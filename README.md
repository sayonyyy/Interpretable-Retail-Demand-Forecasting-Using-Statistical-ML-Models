Here you go, Sayony â€” a **clean, polished, copy-paste-ready GitHub README** version of your project.
Images are added as placeholders so your repo looks professional from day one.

---

# ğŸ“Œ Interpretable Retail Demand Forecasting using Machine Learning & Statistical Models

A complete end-to-end retail demand forecasting system built using **Time-Series Machine Learning, ARIMA/SARIMA models, XGBoost, and SHAP-based interpretability**.

---

## ğŸš€ Project Overview

This project forecasts **weekly retail demand** using the **Walmart Sales Forecasting Dataset**.
The objective is to **compare baseline, statistical, and machine learning models** and identify the most accurate and interpretable forecasting approach for real-world retail planning.

### ğŸ”„ Workflow Summary

* ğŸ“Š Exploratory Time-Series Analysis
* ğŸ§® Feature Engineering (lags, rolling windows, seasonality extraction)
* ğŸ§  Model Building

  * Naive Baseline
  * Linear Regression
  * Random Forest
  * XGBoost
  * ARIMA / SARIMA
* ğŸ“‰ Evaluation: RMSE, MAE, MAPE
* ğŸ” SHAP Interpretability
* ğŸ§¾ Business Insights & Recommendations

---

## ğŸ—‚ï¸ Dataset

**Source:** Walmart Weekly Sales Forecasting Dataset (Kaggle)

### Features Used

* `Store`
* `Dept`
* `Date`
* `Weekly_Sales`
* `IsHoliday`

After filtering:

* **Store:** 1
* **Department:** 1
* **143 weekly observations**

---

## ğŸ“Š 1. Exploratory Data Analysis (EDA)

### Key Observations

* Seasonal spikes around holidays
* No strong long-term trend
* Stable sales between holiday peaks
* High short-term autocorrelation

### Visual Summary (Placeholder)

![Image](https://srjit.github.io/sales-time-series-analysis/images/Validation-plot.png)

![Image](https://raw.githubusercontent.com/liamarguedas/walmart-sales-forecast/main/Summary-Charts/Yearly%20weekly%20sales%20by%20month.png)

Sales patterns indicate:

* Momentum-driven behavior
* Seasonally reactive variations
* Irregular spike-heavy structure

---

## ğŸ› ï¸ 2. Feature Engineering

To convert the time series into a supervised ML problem:

### ğŸ“Œ Lag Features

* `Lag_1` (last week)
* `Lag_2`
* `Lag_4` (1 month prior)
* `Lag_12` (seasonal memory)

### ğŸ“Œ Rolling Window Features

* `Rolling_Mean_4` â€” smooths short-term fluctuations

### ğŸ“Œ Date Features

* `Month`
* `Week`

These features enable machine learning models to capture temporal dependencies effectively.

---

## ğŸ§ª 3. Trainâ€“Test Strategy

* **Trainâ€“Test Split:** 80% train, 20% test
* **No shuffling** â†’ avoids time leakage
* **TimeSeriesSplit CV** used for Random Forest & XGBoost

---

## ğŸ¤– 4. Models Implemented

### ğŸ”¹ Naive Baseline

Predicts
**Next week = Last week (Lag_1)**
Works well because of strong autocorrelation.

### ğŸ”¹ Linear Regression

Simple model; struggles on nonlinear spikes.

### ğŸ”¹ Random Forest

Captures nonlinearity better than linear models.

### ğŸ”¹ XGBoost (ğŸ”¥ Best Model)

Gradient boosting with tuned hyperparameters.
Best performance on **all** metrics:

* Lowest RMSE
* Lowest MAE
* Lowest MAPE

### ğŸ”¹ ARIMA & SARIMA

SARIMA failed due to inconsistent seasonality.
ARIMA improved but still far below ML models.

---

## ğŸ“ˆ 5. Model Performance Comparison

| Model              | RMSE     | MAE     | MAPE      |
| ------------------ | -------- | ------- | --------- |
| Naive              | 1308     | 1077    | 5.73%     |
| Linear Regression  | 3037     | 1455    | 8.45%     |
| Random Forest      | 1336     | 966     | 4.92%     |
| **XGBoost (Best)** | **1114** | **782** | **4.01%** |
| SARIMA             | 14016    | 13310   | 75.32%    |
| ARIMA              | 10455    | 9664    | 55.59%    |

Winner: **XGBoost**

> ~65% improvement over linear regression and significantly beats statistical models.

---

## ğŸ” 6. SHAP Interpretability (Explainable ML)

SHAP was applied to explain XGBoost predictions.

### Key Insights

* **Lag_1** dominates (~57% importance)
* **Lag_2** also important
* `Rolling_Mean_4` raises predictions when recent momentum is high
* `Lag_12` contributes less (weak long-term seasonality)
* Holiday effects inconsistent

Conclusion:
Retail demand is **short-memory driven**, not long-term seasonal.

### SHAP Plot

![SHAP Summary Plot (Global Feature Importance)](notebook/images/shap-summary1.png)


![SHAP Bar Plot (Simple Global Importance)](notebook/images/shap-summary2.png)

![SHAP Force Plot - Decision Plot Alternative (Local Explanation)](notebook/images/shap-summary2.png)
---

## ğŸ§¾ 7. Business Insights

âœ” Recent demand (Lag_1, Lag_2) is the strongest predictor
âœ” Spikes caused by promotions/holidays are inconsistent
âœ” ARIMA/SARIMA struggle with irregular retail behavior
âœ” XGBoost handles spikes + nonlinearity exceptionally well

These findings help with:

* Better inventory planning
* Understanding demand momentum
* Avoiding over-reliance on classical forecasting models

---

## ğŸ§± 8. Tech Stack

* Python
* Pandas
* NumPy
* Matplotlib
* Scikit-learn
* Statsmodels
* XGBoost
* SHAP

---

## ğŸ“‚ Project Structure

```
Retail_Demand_Forecasting/
â”‚â”€â”€ data/
â”‚â”€â”€ notebook/
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ 03_Modeling.ipynb
â”‚   â”œâ”€â”€ 04_Evaluation.ipynb
â”‚   â”œâ”€â”€ 05_Interpretability_SHAP.ipynb
â”‚â”€â”€ README.md
```

---

## ğŸ§  9. Key Learnings

* Time-series forecasting requires **data-aware splitting**
* Lag features outperform complex statistical models
* **Boosting > Bagging > Linear models** for nonlinear patterns
* SHAP is essential for **interpretable ML**
* Retail data is messy â†’ classical ARIMA often fails

---

## ğŸ 10. Final Summary

This project demonstrates a fully interpretable retail forecasting workflow covering:

âœ” EDA
âœ” Feature Engineering
âœ” Model Comparison
âœ” Time-Series CV
âœ” SHAP Explainability
âœ” Business Insights

**XGBoost is the most accurate & reliable forecasting model for retail demand.**

---

## â­ Support

If this repository helped you, please consider **starring the project**!

---


